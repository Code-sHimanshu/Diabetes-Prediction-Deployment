{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96254,"databundleVersionId":11427390,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shimanshusingh/unmasking-fakes-by-himanshu?scriptVersionId=234809632\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install pandas numpy matplotlib seaborn scikit-learn nltk","metadata":{"_uuid":"b523b529-9bd0-45f8-8fe6-db00501da544","_cell_guid":"c4df37f1-05b0-49d1-a615-5e07c067d640","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T21:18:53.51065Z","iopub.execute_input":"2025-04-17T21:18:53.510953Z","iopub.status.idle":"2025-04-17T21:18:58.221548Z","shell.execute_reply.started":"2025-04-17T21:18:53.510935Z","shell.execute_reply":"2025-04-17T21:18:58.220621Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Try multiple classifier imports with fallbacks\ntry:\n    from sklearn.ensemble import RandomForestClassifier\n    classifier = RandomForestClassifier()\nexcept ImportError:\n    try:\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(max_iter=1000)\n        print(\"Using LogisticRegression as fallback\")\n    except ImportError:\n        from sklearn.svm import LinearSVC\n        classifier = LinearSVC()\n        print(\"Using LinearSVC as fallback\")\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/complete.csv')\ntest_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/validate.csv')\n\n# Check available columns in test_df\nprint(\"Columns in test_df:\", test_df.columns.tolist())\n\n# Simple text classification pipeline\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\nX = vectorizer.fit_transform(train_df['Statement'])\ny = train_df['Label']\n\n# Train-test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nclassifier.fit(X_train, y_train)\n\n# Evaluate\nval_predictions = classifier.predict(X_val)\nprint(classification_report(y_val, val_predictions))\nprint(f\"Accuracy: {accuracy_score(y_val, val_predictions):.2f}\")\n\n# Make predictions\ntest_X = vectorizer.transform(test_df['Statement'])\ntest_predictions = classifier.predict(test_X)\n\n# Create submission - using the correct ID column name\n# First try common column names for ID\nid_column = None\nfor possible_id in ['id', 'ID', 'Id', 'statement_id']:\n    if possible_id in test_df.columns:\n        id_column = possible_id\n        break\n\nif id_column is None:\n    # If no ID column found, use index\n    submission = pd.DataFrame({\n        'label': test_predictions\n    })\n    print(\"No ID column found - using index as identifier\")\nelse:\n    submission = pd.DataFrame({\n        'id': test_df[id_column],\n        'label': test_predictions\n    })\n\nsubmission.to_csv('submission1.csv', index=False)\nprint(\"Submission file created!\")","metadata":{"_uuid":"518e770e-7b8a-46df-868c-c290d3f491e9","_cell_guid":"611e23eb-750b-457d-ab2d-a7b67fb51454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T21:18:58.22271Z","iopub.execute_input":"2025-04-17T21:18:58.223049Z","iopub.status.idle":"2025-04-17T21:19:03.730863Z","shell.execute_reply.started":"2025-04-17T21:18:58.223012Z","shell.execute_reply":"2025-04-17T21:19:03.730037Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Columns in test_df: ['ID', 'Statement', 'Subject', 'Speaker', 'Speaker Job Title', 'State', 'Political Party Affiliation', 'True Count', 'False Count', 'Context']\n              precision    recall  f1-score   support\n\n       False       0.67      0.78      0.72       564\n        True       0.61      0.47      0.53       414\n\n    accuracy                           0.65       978\n   macro avg       0.64      0.63      0.63       978\nweighted avg       0.65      0.65      0.64       978\n\nAccuracy: 0.65\nSubmission file created!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# First, let's fix the scikit-learn version\n!pip install --upgrade scikit-learn==1.0.2\nimport sklearn\nprint(\"Scikit-learn version:\", sklearn.__version__)\n\n# Now implement fine-tuning with version-safe code\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Manual grid search implementation (avoids GridSearchCV import issues)\ndef manual_grid_search(X_train, y_train, X_val, y_val):\n    best_score = 0\n    best_params = {}\n    best_model = None\n    \n    # Define parameter combinations to try\n    param_combinations = [\n        {'max_features': 5000, 'ngram_range': (1, 1), 'C': 0.1},\n        {'max_features': 10000, 'ngram_range': (1, 2), 'C': 1},\n        {'max_features': 15000, 'ngram_range': (1, 3), 'C': 10},\n        {'max_features': 10000, 'ngram_range': (1, 2), 'C': 0.1},\n        {'max_features': 15000, 'ngram_range': (1, 2), 'C': 1}\n    ]\n    \n    for params in param_combinations:\n        # Vectorize text\n        vectorizer = TfidfVectorizer(\n            max_features=params['max_features'],\n            ngram_range=params['ngram_range'],\n            stop_words='english'\n        )\n        X_train_vec = vectorizer.fit_transform(X_train)\n        X_val_vec = vectorizer.transform(X_val)\n        \n        # Train model\n        model = LogisticRegression(\n            C=params['C'],\n            max_iter=1000,\n            solver='liblinear',\n            penalty='l2'\n        )\n        model.fit(X_train_vec, y_train)\n        \n        # Evaluate\n        val_predictions = model.predict(X_val_vec)\n        score = accuracy_score(y_val, val_predictions)\n        \n        if score > best_score:\n            best_score = score\n            best_params = params\n            best_model = (vectorizer, model)\n    \n    return best_model, best_params, best_score\n\n# Create train/validation split\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df['Statement'],\n    train_df['Label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Run manual grid search\nbest_model, best_params, best_score = manual_grid_search(X_train, y_train, X_val, y_val)\nvectorizer, model = best_model\n\nprint(\"\\nBest parameters found:\")\nprint(best_params)\nprint(f\"Best validation accuracy: {best_score:.2f}\")\n\n# Evaluate on full training data\nX_full_vec = vectorizer.transform(train_df['Statement'])\ny_full_pred = model.predict(X_full_vec)\nprint(\"\\nFull training set performance:\")\nprint(classification_report(train_df['Label'], y_full_pred))\n\n# Create final submission\ntest_vec = vectorizer.transform(test_df['Statement'])\ntest_predictions = model.predict(test_vec)\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'Label': test_predictions\n})\nsubmission.to_csv('optimized_submission.csv', index=False)\nprint(\"\\nOptimized submission file created!\")","metadata":{"_uuid":"8177a075-5036-4272-8a3e-668720bbc830","_cell_guid":"96ccdcaf-6d19-41bb-bd76-497c16736ba8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T21:19:03.732363Z","iopub.execute_input":"2025-04-17T21:19:03.732594Z","iopub.status.idle":"2025-04-17T21:20:07.506891Z","shell.execute_reply.started":"2025-04-17T21:19:03.732576Z","shell.execute_reply":"2025-04-17T21:20:07.506097Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.0.2\n  Downloading scikit-learn-1.0.2.tar.gz (6.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nScikit-learn version: 1.2.2\n\nBest parameters found:\n{'max_features': 15000, 'ngram_range': (1, 2), 'C': 1}\nBest validation accuracy: 0.65\n\nFull training set performance:\n              precision    recall  f1-score   support\n\n       False       0.81      0.90      0.85      2691\n        True       0.86      0.74      0.79      2196\n\n    accuracy                           0.83      4887\n   macro avg       0.83      0.82      0.82      4887\nweighted avg       0.83      0.83      0.83      4887\n\n\nOptimized submission file created!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# First, reset the environment to stable versions\n!pip install --upgrade scikit-learn==0.24.2 pandas numpy\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/complete.csv')\ntest_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/validate.csv')\n\n# Basic but effective feature engineering\ndef add_simple_features(df):\n    df['word_count'] = df['Statement'].apply(lambda x: len(str(x).split()))\n    df['char_count'] = df['Statement'].apply(lambda x: len(str(x)))\n    df['excl_quest_count'] = df['Statement'].apply(lambda x: str(x).count('!') + str(x).count('?'))\n    return df\n\ntrain_df = add_simple_features(train_df)\ntest_df = add_simple_features(test_df)\n\n# Text vectorization - using only basic parameters\nvectorizer = TfidfVectorizer(\n    max_features=10000,\n    ngram_range=(1, 2),\n    stop_words='english'\n)\n\n# Prepare features\nX_text = vectorizer.fit_transform(train_df['Statement'])\nX_test_text = vectorizer.transform(test_df['Statement'])\n\n# Simple logistic regression model (most stable classifier)\nmodel = LogisticRegression(\n    C=1,\n    max_iter=1000,\n    solver='liblinear',\n    penalty='l2'\n)\n\n# Train model\nmodel.fit(X_text, train_df['Label'])\n\n# Evaluate\ntrain_predictions = model.predict(X_text)\nprint(\"Training Performance:\")\nprint(classification_report(train_df['Label'], train_predictions))\n\n# Create submission\ntest_predictions = model.predict(X_test_text)\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'label': test_predictions\n})\nsubmission.to_csv('stable_submission.csv', index=False)\nprint(\"\\nSubmission file created!\")","metadata":{"_uuid":"bc247099-4da8-4e6f-a752-e13b1d7f61c8","_cell_guid":"f4f36f16-3992-42ca-807c-7e27694fab4d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T21:20:07.507813Z","iopub.execute_input":"2025-04-17T21:20:07.508073Z","iopub.status.idle":"2025-04-17T21:21:08.001376Z","shell.execute_reply.started":"2025-04-17T21:20:07.508039Z","shell.execute_reply":"2025-04-17T21:21:08.000552Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==0.24.2\n  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nTraining Performance:\n              precision    recall  f1-score   support\n\n       False       0.82      0.92      0.87      2691\n        True       0.89      0.75      0.81      2196\n\n    accuracy                           0.85      4887\n   macro avg       0.85      0.84      0.84      4887\nweighted avg       0.85      0.85      0.84      4887\n\n\nSubmission file created!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. Supercharged Feature Engineering (No External Dependencies)\ndef create_power_features(df):\n    # Basic text stats\n    df['word_count'] = df['Statement'].apply(lambda x: len(str(x).split()))\n    df['char_count'] = df['Statement'].apply(lambda x: len(str(x)))\n    df['avg_word_len'] = df['char_count'] / df['word_count']\n    \n    # Advanced punctuation analysis\n    df['excl_quest'] = df['Statement'].apply(lambda x: str(x).count('!') + str(x).count('?'))\n    df['quote_count'] = df['Statement'].apply(lambda x: str(x).count('\"'))\n    df['number_count'] = df['Statement'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n    \n    # Political context features\n    df['is_republican'] = df['Political Party Affiliation'].apply(\n        lambda x: 1 if str(x).lower() == 'republican' else 0)\n    df['is_democrat'] = df['Political Party Affiliation'].apply(\n        lambda x: 1 if str(x).lower() == 'democrat' else 0)\n    \n    # Contextual flags\n    df['has_state'] = df['State'].apply(lambda x: 0 if pd.isna(x) else 1)\n    df['has_job_title'] = df['Speaker Job Title'].apply(lambda x: 0 if pd.isna(x) else 1)\n    \n    return df\n\n# Apply feature engineering\ntrain_df = create_power_features(train_df)\ntest_df = create_power_features(test_df)\n\n# 2. Robust Text Vectorization\nvectorizer = TfidfVectorizer(\n    max_features=20000,\n    ngram_range=(1, 2),\n    stop_words='english',\n    min_df=3,\n    max_df=0.9\n)\n\n# Prepare features\nX_text = vectorizer.fit_transform(train_df['Statement'])\nX_test_text = vectorizer.transform(test_df['Statement'])\n\n# Combine with engineered features\nfeature_cols = ['word_count', 'char_count', 'avg_word_len', 'excl_quest', \n                'quote_count', 'number_count', 'is_republican', 'is_democrat',\n                'has_state', 'has_job_title']\n\nX_engineered = train_df[feature_cols].values\nX_test_engineered = test_df[feature_cols].values\n\n# Final feature matrix\nX_final = np.hstack([X_text.toarray(), X_engineered])\nX_test_final = np.hstack([X_test_text.toarray(), X_test_engineered])\n\n# 3. Optimized Logistic Regression\nmodel = LogisticRegression(\n    C=0.8,\n    max_iter=2000,\n    solver='liblinear',\n    penalty='l2',\n    class_weight='balanced'\n)\n\n# Train model\nmodel.fit(X_final, train_df['Label'])\n\n# Evaluate\ntrain_preds = model.predict(X_final)\nprint(\"Enhanced Training Performance:\")\nprint(classification_report(train_df['Label'], train_preds))\n\n# 4. Create Submission\n# 4. Create Submission with lowercase labels\ntest_preds = model.predict(X_test_final)\n\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'Label': np.where(test_preds == 1, 'true', 'false')  # lowercase labels\n})\nsubmission.to_csv('submissionV3.csv', index=False)\nprint(\"\\n✅ submissionV3.csv file created with correct label format!\")","metadata":{"_uuid":"051fd473-33c6-47ce-9803-d1675e611719","_cell_guid":"cae1b844-a68f-4612-8c02-dffd6b2a8325","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T21:21:08.003268Z","iopub.execute_input":"2025-04-17T21:21:08.004483Z","iopub.status.idle":"2025-04-17T21:21:08.854651Z","shell.execute_reply.started":"2025-04-17T21:21:08.004456Z","shell.execute_reply":"2025-04-17T21:21:08.853904Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Enhanced Training Performance:\n              precision    recall  f1-score   support\n\n       False       0.81      0.77      0.79      2691\n        True       0.73      0.78      0.75      2196\n\n    accuracy                           0.77      4887\n   macro avg       0.77      0.77      0.77      4887\nweighted avg       0.78      0.77      0.77      4887\n\n\n✅ submissionV3.csv file created with correct label format!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"_uuid":"10c2ddf1-a003-49fa-adb4-0494c416db7b","_cell_guid":"8e4b406a-8d85-4ece-8afa-b8b9b93eb1fe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}