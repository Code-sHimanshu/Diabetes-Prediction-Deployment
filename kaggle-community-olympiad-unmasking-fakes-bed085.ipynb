{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96254,"databundleVersionId":11427390,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pandas numpy matplotlib seaborn scikit-learn nltk","metadata":{"_uuid":"b523b529-9bd0-45f8-8fe6-db00501da544","_cell_guid":"c4df37f1-05b0-49d1-a615-5e07c067d640","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:45:52.903562Z","iopub.execute_input":"2025-04-17T20:45:52.903862Z","iopub.status.idle":"2025-04-17T20:45:57.490335Z","shell.execute_reply.started":"2025-04-17T20:45:52.903808Z","shell.execute_reply":"2025-04-17T20:45:57.489272Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Try multiple classifier imports with fallbacks\ntry:\n    from sklearn.ensemble import RandomForestClassifier\n    classifier = RandomForestClassifier()\nexcept ImportError:\n    try:\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(max_iter=1000)\n        print(\"Using LogisticRegression as fallback\")\n    except ImportError:\n        from sklearn.svm import LinearSVC\n        classifier = LinearSVC()\n        print(\"Using LinearSVC as fallback\")\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/complete.csv')\ntest_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/validate.csv')\n\n# Check available columns in test_df\nprint(\"Columns in test_df:\", test_df.columns.tolist())\n\n# Simple text classification pipeline\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\nX = vectorizer.fit_transform(train_df['Statement'])\ny = train_df['Label']\n\n# Train-test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nclassifier.fit(X_train, y_train)\n\n# Evaluate\nval_predictions = classifier.predict(X_val)\nprint(classification_report(y_val, val_predictions))\nprint(f\"Accuracy: {accuracy_score(y_val, val_predictions):.2f}\")\n\n# Make predictions\ntest_X = vectorizer.transform(test_df['Statement'])\ntest_predictions = classifier.predict(test_X)\n\n# Create submission - using the correct ID column name\n# First try common column names for ID\nid_column = None\nfor possible_id in ['id', 'ID', 'Id', 'statement_id']:\n    if possible_id in test_df.columns:\n        id_column = possible_id\n        break\n\nif id_column is None:\n    # If no ID column found, use index\n    submission = pd.DataFrame({\n        'label': test_predictions\n    })\n    print(\"No ID column found - using index as identifier\")\nelse:\n    submission = pd.DataFrame({\n        'id': test_df[id_column],\n        'label': test_predictions\n    })\n\nsubmission.to_csv('submission1.csv', index=False)\nprint(\"Submission file created!\")","metadata":{"_uuid":"518e770e-7b8a-46df-868c-c290d3f491e9","_cell_guid":"611e23eb-750b-457d-ab2d-a7b67fb51454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:50:03.120808Z","iopub.execute_input":"2025-04-17T20:50:03.121114Z","iopub.status.idle":"2025-04-17T20:50:07.917809Z","shell.execute_reply.started":"2025-04-17T20:50:03.121090Z","shell.execute_reply":"2025-04-17T20:50:07.916921Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, let's fix the scikit-learn version\n!pip install --upgrade scikit-learn==1.0.2\nimport sklearn\nprint(\"Scikit-learn version:\", sklearn.__version__)\n\n# Now implement fine-tuning with version-safe code\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Manual grid search implementation (avoids GridSearchCV import issues)\ndef manual_grid_search(X_train, y_train, X_val, y_val):\n    best_score = 0\n    best_params = {}\n    best_model = None\n    \n    # Define parameter combinations to try\n    param_combinations = [\n        {'max_features': 5000, 'ngram_range': (1, 1), 'C': 0.1},\n        {'max_features': 10000, 'ngram_range': (1, 2), 'C': 1},\n        {'max_features': 15000, 'ngram_range': (1, 3), 'C': 10},\n        {'max_features': 10000, 'ngram_range': (1, 2), 'C': 0.1},\n        {'max_features': 15000, 'ngram_range': (1, 2), 'C': 1}\n    ]\n    \n    for params in param_combinations:\n        # Vectorize text\n        vectorizer = TfidfVectorizer(\n            max_features=params['max_features'],\n            ngram_range=params['ngram_range'],\n            stop_words='english'\n        )\n        X_train_vec = vectorizer.fit_transform(X_train)\n        X_val_vec = vectorizer.transform(X_val)\n        \n        # Train model\n        model = LogisticRegression(\n            C=params['C'],\n            max_iter=1000,\n            solver='liblinear',\n            penalty='l2'\n        )\n        model.fit(X_train_vec, y_train)\n        \n        # Evaluate\n        val_predictions = model.predict(X_val_vec)\n        score = accuracy_score(y_val, val_predictions)\n        \n        if score > best_score:\n            best_score = score\n            best_params = params\n            best_model = (vectorizer, model)\n    \n    return best_model, best_params, best_score\n\n# Create train/validation split\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    train_df['Statement'],\n    train_df['Label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Run manual grid search\nbest_model, best_params, best_score = manual_grid_search(X_train, y_train, X_val, y_val)\nvectorizer, model = best_model\n\nprint(\"\\nBest parameters found:\")\nprint(best_params)\nprint(f\"Best validation accuracy: {best_score:.2f}\")\n\n# Evaluate on full training data\nX_full_vec = vectorizer.transform(train_df['Statement'])\ny_full_pred = model.predict(X_full_vec)\nprint(\"\\nFull training set performance:\")\nprint(classification_report(train_df['Label'], y_full_pred))\n\n# Create final submission\ntest_vec = vectorizer.transform(test_df['Statement'])\ntest_predictions = model.predict(test_vec)\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'Label': test_predictions\n})\nsubmission.to_csv('optimized_submission.csv', index=False)\nprint(\"\\nOptimized submission file created!\")","metadata":{"_uuid":"8177a075-5036-4272-8a3e-668720bbc830","_cell_guid":"96ccdcaf-6d19-41bb-bd76-497c16736ba8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:46:03.513862Z","iopub.execute_input":"2025-04-17T20:46:03.514175Z","iopub.status.idle":"2025-04-17T20:47:20.916041Z","shell.execute_reply.started":"2025-04-17T20:46:03.514150Z","shell.execute_reply":"2025-04-17T20:47:20.915269Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, reset the environment to stable versions\n!pip install --upgrade scikit-learn==0.24.2 pandas numpy\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/complete.csv')\ntest_df = pd.read_csv('/kaggle/input/kaggle-community-olympiad-unmasking-fakes/validate.csv')\n\n# Basic but effective feature engineering\ndef add_simple_features(df):\n    df['word_count'] = df['Statement'].apply(lambda x: len(str(x).split()))\n    df['char_count'] = df['Statement'].apply(lambda x: len(str(x)))\n    df['excl_quest_count'] = df['Statement'].apply(lambda x: str(x).count('!') + str(x).count('?'))\n    return df\n\ntrain_df = add_simple_features(train_df)\ntest_df = add_simple_features(test_df)\n\n# Text vectorization - using only basic parameters\nvectorizer = TfidfVectorizer(\n    max_features=10000,\n    ngram_range=(1, 2),\n    stop_words='english'\n)\n\n# Prepare features\nX_text = vectorizer.fit_transform(train_df['Statement'])\nX_test_text = vectorizer.transform(test_df['Statement'])\n\n# Simple logistic regression model (most stable classifier)\nmodel = LogisticRegression(\n    C=1,\n    max_iter=1000,\n    solver='liblinear',\n    penalty='l2'\n)\n\n# Train model\nmodel.fit(X_text, train_df['Label'])\n\n# Evaluate\ntrain_predictions = model.predict(X_text)\nprint(\"Training Performance:\")\nprint(classification_report(train_df['Label'], train_predictions))\n\n# Create submission\ntest_predictions = model.predict(X_test_text)\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'label': test_predictions\n})\nsubmission.to_csv('stable_submission.csv', index=False)\nprint(\"\\nSubmission file created!\")","metadata":{"_uuid":"bc247099-4da8-4e6f-a752-e13b1d7f61c8","_cell_guid":"f4f36f16-3992-42ca-807c-7e27694fab4d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:47:20.917046Z","iopub.execute_input":"2025-04-17T20:47:20.917334Z","iopub.status.idle":"2025-04-17T20:48:36.525893Z","shell.execute_reply.started":"2025-04-17T20:47:20.917299Z","shell.execute_reply":"2025-04-17T20:48:36.524815Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 1. Supercharged Feature Engineering (No External Dependencies)\ndef create_power_features(df):\n    # Basic text stats\n    df['word_count'] = df['Statement'].apply(lambda x: len(str(x).split()))\n    df['char_count'] = df['Statement'].apply(lambda x: len(str(x)))\n    df['avg_word_len'] = df['char_count'] / df['word_count']\n    \n    # Advanced punctuation analysis\n    df['excl_quest'] = df['Statement'].apply(lambda x: str(x).count('!') + str(x).count('?'))\n    df['quote_count'] = df['Statement'].apply(lambda x: str(x).count('\"'))\n    df['number_count'] = df['Statement'].apply(lambda x: sum(c.isdigit() for c in str(x)))\n    \n    # Political context features\n    df['is_republican'] = df['Political Party Affiliation'].apply(\n        lambda x: 1 if str(x).lower() == 'republican' else 0)\n    df['is_democrat'] = df['Political Party Affiliation'].apply(\n        lambda x: 1 if str(x).lower() == 'democrat' else 0)\n    \n    # Contextual flags\n    df['has_state'] = df['State'].apply(lambda x: 0 if pd.isna(x) else 1)\n    df['has_job_title'] = df['Speaker Job Title'].apply(lambda x: 0 if pd.isna(x) else 1)\n    \n    return df\n\n# Apply feature engineering\ntrain_df = create_power_features(train_df)\ntest_df = create_power_features(test_df)\n\n# 2. Robust Text Vectorization\nvectorizer = TfidfVectorizer(\n    max_features=20000,\n    ngram_range=(1, 2),\n    stop_words='english',\n    min_df=3,\n    max_df=0.9\n)\n\n# Prepare features\nX_text = vectorizer.fit_transform(train_df['Statement'])\nX_test_text = vectorizer.transform(test_df['Statement'])\n\n# Combine with engineered features\nfeature_cols = ['word_count', 'char_count', 'avg_word_len', 'excl_quest', \n                'quote_count', 'number_count', 'is_republican', 'is_democrat',\n                'has_state', 'has_job_title']\n\nX_engineered = train_df[feature_cols].values\nX_test_engineered = test_df[feature_cols].values\n\n# Final feature matrix\nX_final = np.hstack([X_text.toarray(), X_engineered])\nX_test_final = np.hstack([X_test_text.toarray(), X_test_engineered])\n\n# 3. Optimized Logistic Regression\nmodel = LogisticRegression(\n    C=0.8,\n    max_iter=2000,\n    solver='liblinear',\n    penalty='l2',\n    class_weight='balanced'\n)\n\n# Train model\nmodel.fit(X_final, train_df['Label'])\n\n# Evaluate\ntrain_preds = model.predict(X_final)\nprint(\"Enhanced Training Performance:\")\nprint(classification_report(train_df['Label'], train_preds))\n\n# 4. Create Submission\n# 4. Create Submission with lowercase labels\ntest_preds = model.predict(X_test_final)\n\nsubmission = pd.DataFrame({\n    'ID': test_df['ID'],\n    'Label': np.where(test_preds == 1, 'true', 'false')  # lowercase labels\n})\nsubmission.to_csv('submissionV3.csv', index=False)\nprint(\"\\nâœ… submissionV3.csv file created with correct label format!\")","metadata":{"_uuid":"051fd473-33c6-47ce-9803-d1675e611719","_cell_guid":"cae1b844-a68f-4612-8c02-dffd6b2a8325","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:57:41.743773Z","iopub.execute_input":"2025-04-17T20:57:41.744076Z","iopub.status.idle":"2025-04-17T20:57:42.603957Z","shell.execute_reply.started":"2025-04-17T20:57:41.744057Z","shell.execute_reply":"2025-04-17T20:57:42.603081Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"10c2ddf1-a003-49fa-adb4-0494c416db7b","_cell_guid":"8e4b406a-8d85-4ece-8afa-b8b9b93eb1fe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}